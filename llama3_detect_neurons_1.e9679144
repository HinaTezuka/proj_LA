Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:29,  9.89s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:19<00:19,  9.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:28<00:09,  9.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  6.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.81s/it]
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
