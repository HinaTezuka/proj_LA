Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:24,  8.32s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:16<00:16,  8.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:24<00:08,  8.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  5.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.63s/it]
LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:25,  8.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:16<00:16,  8.32s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:24<00:08,  8.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  5.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.63s/it]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:05<00:25,  5.04s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:09<00:19,  4.98s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:15<00:15,  5.03s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:20<00:10,  5.06s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:25<00:04,  5.00s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.07s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:27<00:00,  4.55s/it]
