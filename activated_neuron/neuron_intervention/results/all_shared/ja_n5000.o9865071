unfolded pickle
result_main: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.827}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.972}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.974}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.656}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.74}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.708}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.667}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.72}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.771}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.912}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.887}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.736}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.827}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.835}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.77}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.789}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.876}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.712}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.646}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.472}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.827}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.812}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.688}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.974}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.666}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.788}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.609}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.677}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.587}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.764}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.722}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.848}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.764}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.685}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.831}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.455}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.456}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.593}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.602}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.391}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.633}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.632}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.664}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.84}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.989}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.644}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.583}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.655}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.879}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.825}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.982}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.653}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.452}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.77}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.789}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.711}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.791}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.66}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.732}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.82}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.858}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.824}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.911}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.909}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.325}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.286}]
result_comp: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.856}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.986}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.991}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.725}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.771}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.726}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.566}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.74}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.841}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.949}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.972}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.82}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.937}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.93}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.831}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.899}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.933}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.771}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.704}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.525}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.777}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.947}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.806}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.945}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.218}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.875}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.811}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.717}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.614}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.975}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.866}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.881}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.835}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.631}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.836}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.344}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.618}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.728}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.763}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.46}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.735}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.728}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.779}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.896}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.99}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.849}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.642}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.364}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.932}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.88}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.986}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.628}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.475}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.779}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.786}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.711}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.812}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.755}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.718}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.803}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.907}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.887}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.966}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.963}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.367}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.312}]
result_comp_L1_or_L2: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.854}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.986}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.988}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.721}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.715}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.712}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.612}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.691}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.84}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.936}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.936}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.779}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.894}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.891}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.789}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.872}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.914}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.642}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.561}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.493}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.765}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.954}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.688}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.959}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.146}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.827}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.725}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.683}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.603}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.978}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.831}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.814}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.735}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.684}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.822}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.398}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.615}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.692}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.498}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.473}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.731}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.736}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.732}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.871}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.988}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.751}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.627}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.449}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.88}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.806}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.98}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.549}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.367}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.818}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.827}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.75}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.739}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.723}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.712}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.85}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.913}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.876}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.974}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.959}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.263}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.214}]
result_comp_L1_specific: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.883}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.986}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.993}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.729}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.713}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.723}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.622}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.728}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.823}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.94}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.972}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.804}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.933}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.935}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.797}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.914}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.924}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.68}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.589}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.488}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.796}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.956}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.714}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.953}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.244}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.841}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.706}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.698}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.608}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.975}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.832}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.84}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.803}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.684}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.846}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.319}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.588}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.649}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.482}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.424}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.748}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.725}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.762}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.88}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.994}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.78}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.632}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.428}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.899}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.854}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.949}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.51}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.368}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.848}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.779}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.81}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.735}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.755}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.664}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.841}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.914}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.884}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.971}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.956}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.308}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.266}]
                                    Model  ... Accuracy
0   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.827
1   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.972
2   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.974
3   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.656
4   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.740
..                                    ...  ...      ...
62  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.824
63  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.911
64  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.909
65  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.325
66  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.286

[67 rows x 3 columns]
                                    Model  ... Accuracy
0   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.856
1   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.986
2   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.991
3   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.725
4   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.771
..                                    ...  ...      ...
62  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.887
63  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.966
64  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.963
65  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.367
66  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.312

[67 rows x 3 columns]
                                    Model  ... Accuracy
0   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.854
1   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.986
2   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.988
3   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.721
4   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.715
..                                    ...  ...      ...
62  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.876
63  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.974
64  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.959
65  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.263
66  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.214

[67 rows x 3 columns]
                                    Model  ... Accuracy
0   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.883
1   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.986
2   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.993
3   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.729
4   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.713
..                                    ...  ...      ...
62  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.884
63  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.971
64  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.956
65  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.308
66  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.266

[67 rows x 3 columns]
                                   Model  Accuracy
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.732134
                                   Model  Accuracy
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.772687
                                   Model  Accuracy
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.743299
                                   Model  Accuracy
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.752149
completed. saved to csv.

=================================== RESOURCE INFORMATION ===================================
 Requested Resource:
 mem=256000000kb,walltime=168:00:00,ncpus=26,place=pack 
 Used Resource: 
 mem=31570956kb,walltime=34:23:46,ncpus=26,cpupercent=160,vmem=83764400kb
============================================================================================
