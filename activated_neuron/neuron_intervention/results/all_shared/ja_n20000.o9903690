unfolded pickle: act_sum_dict
unfolded pickle: act_freq_base_dict
result_main: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.888}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.972}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.99}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.72}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.715}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.717}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.601}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.718}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.83}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.923}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.943}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.746}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.9}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.891}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.779}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.873}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.905}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.699}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.627}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.453}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.749}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.953}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.722}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.953}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.133}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.854}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.719}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.619}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.561}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.987}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.899}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.842}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.757}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.604}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.854}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.332}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.656}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.701}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.616}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.455}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.724}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.711}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.771}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.856}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.998}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.817}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.657}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.369}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.898}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.825}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.989}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.657}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.374}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.898}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.85}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.676}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.778}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.741}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.427}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.869}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.935}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.896}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.985}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.968}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.267}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.233}]
result_comp: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.821}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.961}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.987}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.697}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.776}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.697}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.561}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.695}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.848}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.928}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.932}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.799}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.88}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.892}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.822}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.824}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.907}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.803}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.734}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.482}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.695}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.918}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.799}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.899}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.293}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.881}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.787}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.695}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.62}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.91}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.821}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.869}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.793}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.6}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.79}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.337}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.682}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.71}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.485}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.45}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.72}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.702}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.775}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.917}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.967}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.759}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.578}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.36}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.928}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.856}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.951}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.558}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.601}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.723}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.881}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.717}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.805}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.745}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.706}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.717}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.889}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.841}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.946}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.941}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.332}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.249}]
result_comp_L1_or_L2: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.875}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.987}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.992}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.683}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.735}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.698}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.641}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.654}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.855}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.903}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.842}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.737}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.821}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.824}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.759}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.794}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.892}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.606}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.534}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.458}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.769}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.948}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.644}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.953}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.278}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.791}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.7}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.618}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.538}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.947}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.827}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.771}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.693}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.672}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.836}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.412}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.573}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.645}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.673}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.469}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.723}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.669}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.711}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.841}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.99}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.664}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.602}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.579}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.849}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.75}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.927}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.575}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.345}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.847}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.782}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.748}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.667}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.722}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.585}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.884}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.922}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.843}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.955}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.953}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.276}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.249}]
result_comp_L1_specific: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.907}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.987}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.992}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.733}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.721}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.715}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.668}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.77}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.818}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.939}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.953}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.797}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.902}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.897}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.812}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.892}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.917}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.661}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.537}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.45}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.785}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.97}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.676}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.974}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.274}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.843}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.712}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.658}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.547}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.964}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.764}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.823}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.745}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.687}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.859}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.402}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.56}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.655}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.524}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.343}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.712}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.692}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.78}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.894}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.994}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.759}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.619}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.508}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.893}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.813}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.963}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.592}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.35}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.816}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.79}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.816}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.71}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.748}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.575}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.856}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.927}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.901}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.973}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.966}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.276}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.257}]
                                    Model  ... Accuracy
0   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.888
1   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.972
2   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.990
3   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.720
4   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.715
..                                    ...  ...      ...
62  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.896
63  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.985
64  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.968
65  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.267
66  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.233

[67 rows x 3 columns]
                                    Model  ... Accuracy
0   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.821
1   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.961
2   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.987
3   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.697
4   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.776
..                                    ...  ...      ...
62  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.841
63  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.946
64  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.941
65  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.332
66  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.249

[67 rows x 3 columns]
                                    Model  ... Accuracy
0   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.875
1   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.987
2   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.992
3   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.683
4   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.735
..                                    ...  ...      ...
62  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.843
63  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.955
64  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.953
65  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.276
66  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.249

[67 rows x 3 columns]
                                    Model  ... Accuracy
0   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.907
1   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.987
2   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.992
3   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.733
4   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.721
..                                    ...  ...      ...
62  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.901
63  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.973
64  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.966
65  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.276
66  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.257

[67 rows x 3 columns]
                                   Model  Accuracy
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.746642
                                   Model  Accuracy
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1   0.74991
                                   Model  Accuracy
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1   0.72694
                                   Model  Accuracy
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.746463
intervention num: 20000
completed. saved to csv.

=================================== RESOURCE INFORMATION ===================================
 Requested Resource:
 mem=256000000kb,walltime=168:00:00,ncpus=26,place=pack 
 Used Resource: 
 mem=11200172kb,walltime=147:59:21,ncpus=26,cpupercent=166,vmem=48304060kb
============================================================================================
