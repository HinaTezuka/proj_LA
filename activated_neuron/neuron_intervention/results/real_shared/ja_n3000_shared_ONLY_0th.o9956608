unfolded pickle: act_sum_dict
unfolded pickle: act_freq_base_dict
unfolded pickle: act_freq_base_dict
43387
result_main: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.888}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.981}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.994}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.725}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.736}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.71}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.571}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.746}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.822}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.952}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.98}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.817}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.949}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.943}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.825}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.922}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.925}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.772}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.673}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.519}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.795}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.945}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.77}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.956}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.207}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.872}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.784}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.71}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.617}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.984}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.861}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.883}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.831}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.639}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.861}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.387}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.626}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.707}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.72}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.479}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.763}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.735}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.742}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.912}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.99}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.772}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.645}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.389}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.934}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.893}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.99}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.65}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.451}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.869}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.8}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.747}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.804}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.75}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.729}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.806}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.924}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.919}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.973}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.968}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.303}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.258}]
result_shared_non_translation: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.844}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.971}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.979}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.691}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.813}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.731}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.58}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.754}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.736}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.942}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.937}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.764}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.907}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.872}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.795}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.849}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.903}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.738}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.673}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.487}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.831}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.788}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.744}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.967}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.754}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.814}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.659}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.708}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.62}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.891}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.836}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.862}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.824}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.703}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.851}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.473}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.475}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.642}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.873}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.501}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.632}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.649}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.736}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.882}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.997}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.723}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.592}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.477}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.919}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.876}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.978}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.68}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.413}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.772}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.899}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.716}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.812}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.696}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.728}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.763}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.849}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.822}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.932}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.92}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.357}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.285}]
result_comp: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.869}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.981}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.993}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.731}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.76}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.732}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.592}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.721}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.851}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.951}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.977}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.818}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.938}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.931}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.81}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.923}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.918}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.754}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.708}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.513}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.768}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.945}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.791}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.963}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.202}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.875}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.801}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.71}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.629}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.984}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.87}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.879}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.837}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.652}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.849}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.374}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.647}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.726}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.752}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.519}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.772}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.734}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.794}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.915}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.995}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.82}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.64}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.384}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.931}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.891}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.995}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.649}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.454}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.852}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.793}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.74}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.829}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.748}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.759}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.849}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.928}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.911}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.981}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.976}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.353}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.275}]
result_comp_L1_or_L2: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.851}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.991}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.99}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.748}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.74}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.718}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.617}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.719}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.827}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.936}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.963}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.791}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.915}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.902}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.796}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.895}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.922}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.697}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.6}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.517}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.756}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.953}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.705}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.962}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.222}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.841}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.748}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.702}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.631}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.985}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.841}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.841}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.746}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.676}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.845}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.415}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.607}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.695}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.568}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.497}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.748}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.739}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.755}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.879}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.99}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.773}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.638}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.433}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.902}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.848}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.99}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.601}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.389}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.811}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.813}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.786}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.754}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.749}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.692}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.838}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.93}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.873}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.97}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.958}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.322}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.27}]
result_comp_L1_specific: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.891}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.986}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.992}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.728}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.726}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.727}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.629}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.737}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.825}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.943}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.974}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.812}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.93}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.922}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.796}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.909}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.923}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.681}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.599}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.488}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.799}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.957}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.743}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.954}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.188}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.856}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.738}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.692}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.614}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.978}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.839}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.855}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.804}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.669}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.841}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.383}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.603}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.678}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.602}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.476}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.734}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.721}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.765}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.893}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.991}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.801}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.632}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.398}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.907}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.868}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.981}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.557}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.381}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.866}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.803}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.805}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.753}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.767}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.696}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.841}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.931}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.903}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.978}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.969}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.312}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.269}]
                                    Model  ... Accuracy
0   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.888
1   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.981
2   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.994
3   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.725
4   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.736
..                                    ...  ...      ...
62  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.919
63  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.973
64  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.968
65  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.303
66  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.258

[67 rows x 3 columns]
                                    Model  ... Accuracy
0   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.844
1   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.971
2   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.979
3   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.691
4   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.813
..                                    ...  ...      ...
62  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.822
63  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.932
64  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.920
65  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.357
66  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.285

[67 rows x 3 columns]
                                    Model  ... Accuracy
0   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.869
1   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.981
2   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.993
3   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.731
4   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.760
..                                    ...  ...      ...
62  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.911
63  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.981
64  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.976
65  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.353
66  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.275

[67 rows x 3 columns]
                                    Model  ... Accuracy
0   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.851
1   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.991
2   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.990
3   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.748
4   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.740
..                                    ...  ...      ...
62  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.873
63  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.970
64  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.958
65  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.322
66  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.270

[67 rows x 3 columns]
                                    Model  ... Accuracy
0   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.891
1   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.986
2   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.992
3   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.728
4   tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.726
..                                    ...  ...      ...
62  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.903
63  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.978
64  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.969
65  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.312
66  tokyotech-llm/Llama-3-Swallow-8B-v0.1  ...    0.269

[67 rows x 3 columns]
                                   Model  Accuracy
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.773134
                                   Model  Accuracy
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.759507
                                   Model  Accuracy
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.779284
                                   Model  Accuracy
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.758537
                                   Model  Accuracy
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.761328
intervention num: 3000
THRESHOLD: 0
count_shared_ONLY: 43387
completed. saved to csv.

=================================== RESOURCE INFORMATION ===================================
 Requested Resource:
 mem=256000000kb,walltime=168:00:00,ncpus=26,place=pack
 Used Resource:
 mem=32704016kb,walltime=30:48:07,ncpus=26,cpupercent=100,vmem=49484460kb
============================================================================================
